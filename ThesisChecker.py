import fitz  # PyMuPDF
import tiktoken
import ollama
from typing import List
import time
from datetime import timedelta


def extract_text_from_pdf(pdf_path: str) -> str:
    """
    Extrahuje text z PDF souboru.
    """
    print("ğŸ“– ÄŒtu PDF soubor...")
    start_time = time.time()

    text = ""
    try:
        doc = fitz.open(pdf_path)
        for page_num in range(len(doc)):
            page = doc.load_page(page_num)
            text += page.get_text("text") + "\n"
        doc.close()

        end_time = time.time()
        print(f"âœ… NaÄteno {len(text)} znakÅ¯ za {timedelta(seconds=end_time - start_time)}")
        return text

    except Exception as e:
        end_time = time.time()
        print(f"âŒ Chyba pÅ™i ÄtenÃ­ PDF ({timedelta(seconds=end_time - start_time)}): {e}")
        return ""


def tokenize_text(text: str, max_tokens: int = 3500, encoding_name: str = "cl100k_base") -> List[str]:
    """
    RozdÄ›lÃ­ text na ÄÃ¡sti podle maximÃ¡lnÃ­ho poÄtu tokenÅ¯.
    SnaÅ¾Ã­ se Å™ezat na koncÃ­ch vÄ›t.
    """
    print("âœ‚ï¸ RozdÄ›luji text na ÄÃ¡sti...")
    start_time = time.time()

    encoding = tiktoken.get_encoding(encoding_name)
    all_tokens = encoding.encode(text)
    total_tokens = len(all_tokens)

    parts = []
    start_index = 0

    while start_index < total_tokens:
        end_index = min(start_index + max_tokens, total_tokens)

        # Pokud nejsme na konci, najdeme hezkÃ© mÃ­sto pro Å™ez
        if end_index < total_tokens:
            current_tokens = all_tokens[start_index:end_index]
            current_text = encoding.decode(current_tokens)

            # HledÃ¡me pÅ™irozenÃ© konce (teÄka, novÃ½ Å™Ã¡dek)
            last_sentence_end = max(
                current_text.rfind('.'),
                current_text.rfind('!'),
                current_text.rfind('?'),
                current_text.rfind('\n\n'),
                current_text.rfind('\n')
            )

            # Pokud jsme naÅ¡li dobrÃ© mÃ­sto pro rozdÄ›lenÃ­
            if last_sentence_end != -1 and last_sentence_end > len(current_text) * 0.6:
                adjusted_text = current_text[:last_sentence_end + 1]
                adjusted_tokens = encoding.encode(adjusted_text)
                end_index = start_index + len(adjusted_tokens)

        # VytvoÅ™Ã­me ÄÃ¡st textu
        part_tokens = all_tokens[start_index:end_index]
        part_text = encoding.decode(part_tokens)

        parts.append(part_text)
        start_index = end_index

    end_time = time.time()
    print(f"âœ… Text rozdÄ›len na {len(parts)} ÄÃ¡stÃ­ za {timedelta(seconds=end_time - start_time)}")
    return parts


def check_grammar_with_ollama(text_chunks: List[str], model_name: str = "jobautomation/OpenEuroLLM-Czech") -> List[str]:
    """
    PoÅ¡le ÄÃ¡sti textu modelu ke kontrole.
    """
    print("ğŸš€ ZaÄÃ­nÃ¡m kontrolu gramatiky...")
    overall_start = time.time()
    results = []

    for i, chunk in enumerate(text_chunks):
        chunk_start = time.time()
        print(f"ğŸ” Kontroluji ÄÃ¡st {i + 1}/{len(text_chunks)}...", end=" ", flush=True)

        try:
            response = ollama.chat(
                model=model_name,
                messages=[{
                    'role': 'user',
                    'content': f"""Zkontroluj tuto ÄÃ¡st textu z pohledu:
                    1. Gramatiky â€“ vypiÅ¡ nejÄastÄ›jÅ¡Ã­ chyby v ÄÃ¡sti: [ÄŒÃ­slo Å™Ã¡dku, PÅ¯vodnÃ­ text, Oprava, Typ chyby]
                    2. Stylistiky â€“ identifikuj odstavce, kterÃ© na sebe nenavazujÃ­ (uvedi ÄÃ­sla odstavcÅ¯ a dÅ¯vod)
                    3. Odbornosti v oboru ergoterapie â€“ vypiÅ¡ vÅ¡echny pasÃ¡Å¾e, kterÃ© jsou odbornÄ› nesprÃ¡vnÃ©, s vysvÄ›tlenÃ­m.

                    Text: {chunk}

                    PoznÃ¡mka: Ignoruj abstrakt, seznam literatury, pÅ™Ã­lohy, obsah, seznam obrÃ¡zkÅ¯ a tabulek.
                    OdpovÄ›Ä dej struÄnÄ›, v ÄeÅ¡tinÄ›.
                    
                    Na ÃºplnÃ½ zÃ¡ver tvÃ©ho hodnocenÃ­ pÅ™Ã­dej celkovÃ© shrnutÃ­ s procentuÃ¡lnÃ­m vyjÃ¡dÅ™enÃ­m kvality textu z hlediska gramatiky, stylistiky a odbornosti (0-100%). BuÄ konkrÃ©tnÃ­ a vÄ›cnÃ½ a muÅ¾eÅ¡ zahrnout i nÃ¡vrhy na zlepÅ¡enÃ­.
                    
                    """
                }]
            )
            results.append(response['message']['content'])

            chunk_end = time.time()
            chunk_time = chunk_end - chunk_start
            print(f"hotovo za {chunk_time:.1f}s")

        except Exception as e:
            chunk_end = time.time()
            chunk_time = chunk_end - chunk_start
            print(f"âŒ Chyba za {chunk_time:.1f}s: {e}")
            results.append(f"CHYBA: {e}")

    overall_end = time.time()
    total_time = overall_end - overall_start
    avg_time = total_time / len(text_chunks) if text_chunks else 0

    print(f"âœ… Kontrola dokonÄena za {timedelta(seconds=total_time)}")
    print(f"ğŸ“Š PrÅ¯mÄ›rnÄ› {avg_time:.1f}s na ÄÃ¡st")

    return results


def main(pdf_path: str):
    """
    HlavnÃ­ funkce: PDF â†’ Text â†’ Tokenizace â†’ Kontrola
    """
    print("=" * 60)
    print("ğŸ¤– SPUÅ TÄšNÃ KONTROLY GRAMATIKY")
    print("=" * 60)

    total_start_time = time.time()

    # FÃ¡ze 1: Extrakce textu z PDF
    text = extract_text_from_pdf(pdf_path)

    if not text:
        print("âŒ NepodaÅ™ilo se naÄÃ­st text z PDF")
        return

    # FÃ¡ze 2: Tokenizace
    chunks = tokenize_text(text, max_tokens=3000)

    # FÃ¡ze 3: Kontrola gramatiky
    results = check_grammar_with_ollama(chunks)

    # FÃ¡ze 4: UloÅ¾enÃ­ vÃ½sledkÅ¯
    print("ğŸ’¾ UklÃ¡dÃ¡m vÃ½sledky...")
    save_start = time.time()

    with open("vysledky_kontroly.txt", "w", encoding="utf-8") as f:
        for i, result in enumerate(results):
            f.write(f"\n{'=' * 50}\n")
            f.write(f"ÄŒÃST {i + 1}\n")
            f.write(f"{'=' * 50}\n\n")
            f.write(result + "\n")

    save_end = time.time()

    # CelkovÃ© statistiky
    total_end_time = time.time()
    total_duration = total_end_time - total_start_time

    print("=" * 60)
    print("ğŸ“Š CELKOVÃ‰ STATISTIKY")
    print("=" * 60)
    print(f"CelkovÃ½ Äas: {timedelta(seconds=total_duration)}")
    print(f"PoÄet ÄÃ¡stÃ­: {len(chunks)}")
    print(f"PrÅ¯mÄ›rnÃ½ Äas na ÄÃ¡st: {total_duration / len(chunks):.1f}s" if chunks else "N/A")
    print(f"ÄŒas uloÅ¾enÃ­: {timedelta(seconds=save_end - save_start)}")
    print(f"âœ… Hotovo! VÃ½sledky uloÅ¾eny v 'vysledky_kontroly.txt'")
    print("=" * 60)


# SpuÅ¡tÄ›nÃ­
if __name__ == "__main__":
    #main("130416806.pdf")  # ğŸ‘ˆ ZDE NAZEV TVÃ‰HO SOUBORU
    main("Lopatka.pdf")  # ğŸ‘ˆ ZDE NAZEV TVÃ‰HO SOUBORU